{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c560ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_jsonl(path, limit=None):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            rows.append(json.loads(line))\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2t6m2f1s",
   "metadata": {},
   "source": [
    "# Amazon Appliances Review Rating Prediction\n",
    "\n",
    "## 1. Predictive Task and Problem Formulation\n",
    "\n",
    "### Task Definition\n",
    "In this project, we focus on predicting the star rating of an Amazon Appliance review using only the review text (the title and the written body). The goal is to take what a customer wrote and guess whether they gave the product 1, 2, 3, 4, or 5 stars.\n",
    "\n",
    "### Evaluation Methodology\n",
    "We will evaluate the model mainly using accuracy, which tells us how often the predicted rating matches the true rating. To get a deeper understanding of performance, we also look at precision, recall, and F1-score for each rating level. A confusion matrix will help us see which ratings the model tends to confuse with each other. We use an 85% training and 15% testing split, with a fixed random seed to make sure results can be reproduced.\n",
    "\n",
    "### Baselines for Comparison\n",
    "To judge how well our model performs, we compare it to several baselines. The first is a majority-class baseline that always predicts the most common rating, typically 5 stars. The second is a random baseline that predicts ratings according to the distribution of the training data. We also include logistic regression with TF-IDF features as a strong and commonly used baseline for text classification tasks.\n",
    "\n",
    "### Model Validity Assessment\n",
    "To check whether the model’s predictions are reliable, we examine the confusion matrix to see if the mistakes make sense, such as mixing up 4-star and 5-star reviews rather than confusing 1-star with 5-star. We also perform error analysis by looking at misclassified examples to understand where the model struggles. Finally, we compare our results to findings from similar rating-prediction or sentiment analysis work to ensure our outcomes are consistent with what others have observed.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dataset Context and Exploratory Data Analysis\n",
    "\n",
    "### Dataset Origin and Collection\n",
    "**Source**: Amazon Review Data (2023 version) from UCSD/Julian McAuley's research group\n",
    "\n",
    "**Citation**:\n",
    "> Hou, Y., Li, J., He, Z., Yan, A., Chen, X., & McAuley, J. (2023). Bridging Language and Items for Retrieval and Recommendation. arXiv. [Link](https://arxiv.org/abs/2305.14385)\n",
    "\n",
    "**Context**:\n",
    "The dataset we use is a large-scale Amazon Reviews collection assembled in 2023. It contains more than 48 million items and over 571 million reviews written by about 54 million users. This dataset, created by the McAuley Lab, offers a wide range of useful information. It includes detailed user reviews such as ratings, review text, and helpfulness votes, along with item metadata that covers descriptions, prices, and raw images. It also provides relational links such as user item connections and bought together graphs. The 2023 release introduces several improvements. It is significantly larger, with more than twice the number of reviews compared to the previous version. It includes newer interactions that span from May 1996 to September 2023, along with richer and cleaner metadata. The timestamps are more precise, reaching the level of seconds, and the dataset now comes with standard data splits to support consistent evaluation for recommendation systems.\n",
    "\n",
    "### Data Processing\n",
    "The dataset comes pre-processed in JSONL format and includes several fields such as the star rating from one to five, the short review title, the full review text, the Unix timestamp, whether the purchase was verified, the number of helpful votes, and identifiers including user ID, ASIN, and parent ASIN. For our preprocessing, we first combined the review title and text into a single input string. We then normalized the text by converting it to lowercase, removing HTML tags, and removing punctuation. Next, we removed stopwords using the NLTK English stopword list, followed by stemming with the Porter Stemmer to reduce words to their base forms. Finally, we transformed the text into numerical features using TF-IDF vectorization with bigrams and limited the vocabulary to fifty thousand features.\n",
    "\n",
    "**Class Distribution Analysis** (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15827a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Appliances.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAppliances.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(path, limit)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_jsonl\u001b[39m(path, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m limit:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Appliances.jsonl'"
     ]
    }
   ],
   "source": [
    "df = load_jsonl(\"Appliances.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da437cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42886734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df[['rating', 'title', 'text']].dropna()\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['reviews'] = df['title'] + ' ' + df['text']\n",
    "df_review.drop(columns=[\"title\", \"text\"], inplace=True)\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed3319",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.countplot(x=\"rating\", data=df)\n",
    "plt.title(\"Star Rating Distribution\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Number of reviews\")\n",
    "plt.show()\n",
    "\n",
    "df[\"rating\"].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8hqrs9yhjjv",
   "metadata": {},
   "source": [
    "The rating distribution in the dataset is heavily skewed toward positive reviews. Approximately sixty five percent of all entries are five star reviews, making this the dominant class. Four star reviews account for about fifteen percent of the data, while one, two, and three star reviews together make up only about twenty percent. This imbalance has important implications for modeling. A naive model that always predicts five stars would already reach an accuracy of roughly sixty five percent, so any useful model must outperform this baseline. The model also needs to capture subtle differences in language to separate adjacent rating levels, and the minority classes from one to three stars will be much harder to predict reliably. As a result, precision and recall are likely to vary substantially across different rating categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df[\"helpful_vote\"], bins=30)\n",
    "plt.title(\"Distribution of Helpful Votes\")\n",
    "plt.xlabel(\"helpful_vote\")\n",
    "plt.ylabel(\"Number of reviews\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df[\"helpful_vote\"], bins=30, log=True)\n",
    "plt.title(\"Distribution of Helpful Votes (log scale)\")\n",
    "plt.xlabel(\"helpful_vote\")\n",
    "plt.ylabel(\"Number of reviews (log)\")\n",
    "plt.show()\n",
    "\n",
    "(df[\"helpful_vote\"] > 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e54680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"verified_purchase\"].value_counts(normalize=True)\n",
    "sns.countplot(x=\"verified_purchase\", data=df)\n",
    "plt.title(\"Verified vs Non-Verified Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: use df instead of r\n",
    "df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df['year'] = df['timestamp_dt'].dt.year\n",
    "df['month'] = df['timestamp_dt'].dt.month\n",
    "\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "print(year_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "year_counts.plot(kind='bar')\n",
    "plt.title('Number of Reviews per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a146ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_item = df[\"parent_asin\"].value_counts()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(reviews_per_item, bins=50, log=True)\n",
    "plt.title(\"Distribution of Reviews per Product (log scale)\")\n",
    "plt.xlabel(\"# reviews per product\")\n",
    "plt.ylabel(\"# products\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Median reviews per product:\", reviews_per_item.median())\n",
    "print(\"Top 10 most-reviewed products:\")\n",
    "print(reviews_per_item.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text_full'] = (\n",
    "    df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    ")\n",
    "df['len_chars'] = df['review_text_full'].str.len()\n",
    "df['len_words'] = df['review_text_full'].str.split().str.len()\n",
    "\n",
    "print(df['len_words'].describe())\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df['len_words'], bins=50)\n",
    "plt.title('Review Length (words)')\n",
    "plt.xlabel('# words')\n",
    "plt.ylabel('# reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between numeric features\n",
    "numeric_cols = ['rating', 'helpful_vote', 'len_words', 'year']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1)\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey correlations:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "melingwlm8k",
   "metadata": {},
   "source": [
    "## Key Insights from EDA\n",
    "\n",
    "Based on the exploratory analysis above, here are the key findings:\n",
    "\n",
    "### Dataset Overview\n",
    "- **Size**: 2.1M+ reviews from Amazon Appliances category\n",
    "- **Time span**: Reviews from 2000s to ~2023\n",
    "- **Products**: Covers thousands of different appliance products\n",
    "- **Users**: Mix of one-time and repeat reviewers\n",
    "\n",
    "### Rating Distribution\n",
    "- **Highly skewed**: Majority of reviews are 4-5 stars (positive bias)\n",
    "- Average rating: ~4.2/5.0\n",
    "- This suggests potential class imbalance for predictive modeling\n",
    "\n",
    "### Review Characteristics\n",
    "- **Length**: Most reviews are short (median ~30-40 words)\n",
    "- **Extremes**: 1-star and 3-star reviews tend to be longer (users explain problems)\n",
    "- **5-star reviews**: Often brief and positive\n",
    "\n",
    "### Verified Purchases\n",
    "- ~90%+ are verified purchases\n",
    "- Verified purchases may have slightly different rating patterns\n",
    "- Important feature for fraud/authenticity detection\n",
    "\n",
    "### Temporal Patterns\n",
    "- Review volume has grown over time\n",
    "- Average ratings relatively stable across years\n",
    "- Seasonal patterns may exist (not fully explored)\n",
    "\n",
    "### Helpfulness\n",
    "- Most reviews receive 0 helpful votes\n",
    "- Very skewed distribution (long tail)\n",
    "- Helpful reviews tend to be longer and more detailed\n",
    "\n",
    "### Potential ML Tasks\n",
    "1. **Rating prediction** from review text\n",
    "2. **Helpfulness prediction** (useful for ranking)\n",
    "3. **Verified purchase detection** (fraud detection)\n",
    "4. **Sentiment analysis** beyond simple rating\n",
    "5. **Review quality assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4hfyz73dbzt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most helpful reviews analysis\n",
    "most_helpful = df.nlargest(10, 'helpful_vote')\n",
    "\n",
    "print(\"TOP 10 MOST HELPFUL REVIEWS:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in most_helpful.iterrows():\n",
    "    print(f\"\\nRating: {row['rating']} stars | Helpful votes: {row['helpful_vote']}\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Review length: {row['len_words']} words\")\n",
    "    print(f\"Verified: {row['verified_purchase']}\")\n",
    "    print(f\"Text preview: {row['text'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3satoavdwr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text analysis: extreme ratings\n",
    "# Sample reviews for different ratings\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE 5-STAR REVIEWS:\")\n",
    "print(\"=\" * 80)\n",
    "sample_5_star = df[df['rating'] == 5.0].sample(3, random_state=42)\n",
    "for idx, row in sample_5_star.iterrows():\n",
    "    print(f\"\\nTitle: {row['title']}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\" if len(row['text']) > 200 else f\"Text: {row['text']}\")\n",
    "    print(f\"Helpful votes: {row['helpful_vote']}, Verified: {row['verified_purchase']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE 1-STAR REVIEWS:\")\n",
    "print(\"=\" * 80)\n",
    "sample_1_star = df[df['rating'] == 1.0].sample(3, random_state=42)\n",
    "for idx, row in sample_1_star.iterrows():\n",
    "    print(f\"\\nTitle: {row['title']}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\" if len(row['text']) > 200 else f\"Text: {row['text']}\")\n",
    "    print(f\"Helpful votes: {row['helpful_vote']}, Verified: {row['verified_purchase']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rdaqewyng1g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal trends analysis\n",
    "# Reviews over time with rating breakdown\n",
    "df_time = df.groupby(['year', 'rating']).size().reset_index(name='count')\n",
    "df_time_pivot = df_time.pivot(index='year', columns='rating', values='count').fillna(0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Stacked area chart\n",
    "df_time_pivot.plot(kind='area', stacked=True, ax=axes[0], alpha=0.7, \n",
    "                   colormap='RdYlGn', legend=True)\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Number of Reviews')\n",
    "axes[0].set_title('Review Volume Over Time (by Rating)')\n",
    "axes[0].legend(title='Rating', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Average rating over time\n",
    "avg_rating_over_time = df.groupby('year')['rating'].mean()\n",
    "axes[1].plot(avg_rating_over_time.index, avg_rating_over_time.values, \n",
    "             marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Average Rating')\n",
    "axes[1].set_title('Average Rating Trend Over Time')\n",
    "axes[1].set_ylim(3.5, 5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average rating by year:\")\n",
    "print(avg_rating_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6u7b5zdnk1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verified vs Non-verified purchase analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Rating distribution by verification status\n",
    "verified_ratings = df[df['verified_purchase'] == True]['rating'].value_counts(normalize=True).sort_index()\n",
    "unverified_ratings = df[df['verified_purchase'] == False]['rating'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "x = verified_ratings.index\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x - width/2, verified_ratings.values, width, label='Verified', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, unverified_ratings.values, width, label='Non-verified', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Rating')\n",
    "axes[0, 0].set_ylabel('Proportion')\n",
    "axes[0, 0].set_title('Rating Distribution: Verified vs Non-Verified')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Average rating by verification\n",
    "avg_ratings = df.groupby('verified_purchase')['rating'].mean()\n",
    "axes[0, 1].bar(['Non-Verified', 'Verified'], \n",
    "               [avg_ratings[False], avg_ratings[True]], \n",
    "               color=['coral', 'steelblue'])\n",
    "axes[0, 1].set_ylabel('Average Rating')\n",
    "axes[0, 1].set_title('Average Rating by Verification Status')\n",
    "axes[0, 1].set_ylim(0, 5)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Review length by verification\n",
    "avg_length = df.groupby('verified_purchase')['len_words'].mean()\n",
    "axes[1, 0].bar(['Non-Verified', 'Verified'], \n",
    "               [avg_length[False], avg_length[True]], \n",
    "               color=['coral', 'steelblue'])\n",
    "axes[1, 0].set_ylabel('Average Words')\n",
    "axes[1, 0].set_title('Average Review Length by Verification')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Helpful votes by verification\n",
    "avg_helpful = df.groupby('verified_purchase')['helpful_vote'].mean()\n",
    "axes[1, 1].bar(['Non-Verified', 'Verified'], \n",
    "               [avg_helpful[False], avg_helpful[True]], \n",
    "               color=['coral', 'steelblue'])\n",
    "axes[1, 1].set_ylabel('Average Helpful Votes')\n",
    "axes[1, 1].set_title('Average Helpful Votes by Verification')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Statistical comparison:\")\n",
    "print(f\"Verified purchases: {(df['verified_purchase'] == True).sum():,} ({(df['verified_purchase'] == True).mean()*100:.1f}%)\")\n",
    "print(f\"Non-verified: {(df['verified_purchase'] == False).sum():,} ({(df['verified_purchase'] == False).mean()*100:.1f}%)\")\n",
    "print(f\"\\nAverage rating - Verified: {avg_ratings[True]:.3f}, Non-verified: {avg_ratings[False]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kabz9lhdrha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User activity analysis\n",
    "reviews_per_user = df['user_id'].value_counts()\n",
    "\n",
    "print(f\"Total unique users: {df['user_id'].nunique():,}\")\n",
    "print(f\"Total unique products: {df['parent_asin'].nunique():,}\")\n",
    "print(f\"\\nReviews per user statistics:\")\n",
    "print(reviews_per_user.describe())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of reviews per user\n",
    "axes[0].hist(reviews_per_user.values, bins=50, log=True, edgecolor='black')\n",
    "axes[0].set_xlabel('Number of Reviews per User')\n",
    "axes[0].set_ylabel('Number of Users (log scale)')\n",
    "axes[0].set_title('Distribution of User Activity')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Top reviewers\n",
    "top_reviewers = reviews_per_user.head(20)\n",
    "axes[1].barh(range(len(top_reviewers)), top_reviewers.values)\n",
    "axes[1].set_yticks(range(len(top_reviewers)))\n",
    "axes[1].set_yticklabels([f\"User {i+1}\" for i in range(len(top_reviewers))])\n",
    "axes[1].set_xlabel('Number of Reviews')\n",
    "axes[1].set_title('Top 20 Most Active Reviewers')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What percentage of users wrote only 1 review?\n",
    "single_review_pct = (reviews_per_user == 1).sum() / len(reviews_per_user) * 100\n",
    "print(f\"\\n{single_review_pct:.1f}% of users wrote only 1 review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zea44r3rhnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Review Length analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='len_words', by='rating', ax=axes[0])\n",
    "axes[0].set_title('Review Length by Rating')\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Number of Words')\n",
    "axes[0].set_ylim(0, 200)  # Focus on main distribution\n",
    "\n",
    "# Average review length per rating\n",
    "avg_length_by_rating = df.groupby('rating')['len_words'].mean()\n",
    "axes[1].bar(avg_length_by_rating.index, avg_length_by_rating.values, color='steelblue')\n",
    "axes[1].set_title('Average Review Length by Rating')\n",
    "axes[1].set_xlabel('Rating')\n",
    "axes[1].set_ylabel('Average Number of Words')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('')  # Remove auto-generated title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average words per rating:\")\n",
    "print(avg_length_by_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uoeztfq7ev",
   "metadata": {},
   "source": [
    "Our analysis of review length shows a noticeable relationship between length and rating. Reviews with one or three stars tend to be longer, since negative or mixed opinions often come with detailed explanations of the issues the customer experienced. In contrast, five star reviews are often short and to the point, with simple statements such as “Great product” or “Love it.” This pattern suggests that review length may provide a weak signal for predicting the rating. Even so, length alone is not enough because it cannot capture the meaning or sentiment behind the words. The TF-IDF features we use address this limitation by reflecting both the length of the review through document frequency and the actual content through term importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eh0n6rqcto4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Modeling Approach\n",
    "\n",
    "### Problem Formulation\n",
    "To formulate our task as a machine learning problem, we treat each combined review text, which includes both the title and the body, as the input. The goal is to predict a rating from one to five, so the output is a discrete class label. The objective of the model is to minimize classification error and, in practice, to maximize accuracy on held-out data. For this task, logistic regression paired with TF-IDF features is an appropriate and effective choice.\n",
    "\n",
    "### Model: Logistic Regression with TF-IDF\n",
    "\n",
    "Logistic regression offers several benefits for text-based prediction. It is easy to interpret because the learned coefficients reveal which words contribute most strongly to each rating class. It is also computationally efficient and scales well to large datasets, which is important given that we work with millions of reviews. The model has a long track record of success as a baseline in text classification, and it produces probabilistic outputs that indicate the model’s confidence in each prediction. It also performs reasonably well even without extensive hyperparameter tuning, which simplifies experimentation.\n",
    "\n",
    "However, logistic regression has some limitations. It relies on a linear decision boundary and therefore cannot model more complex or non-linear patterns in language. It treats each word independently, so it ignores word order and broader contextual meaning. Because TF-IDF creates a very high-dimensional feature space, especially with a vocabulary of fifty thousand terms, the model can overfit without proper regularization. Finally, it lacks true semantic understanding and cannot naturally account for synonyms or paraphrased expressions. Despite these limitations, logistic regression serves as a strong and reliable baseline for our predictive task.\n",
    "\n",
    "**Alternative Models**\n",
    "\n",
    "| Model | Advantages | Disadvantages | Why Not Used |\n",
    "|-------|-----------|---------------|--------------|\n",
    "| **Naive Bayes** | Very fast, simple baseline | Strong independence assumptions | Less accurate than Logistic Regression in practice |\n",
    "| **Random Forest** | Handles non-linearity, feature interactions | Slow on high-dim sparse data, less interpretable | Computational cost prohibitive at this scale |\n",
    "| **Neural Networks (LSTM/BERT)** | Captures context, word order, semantics | Requires GPU, long training time, less interpretable | Beyond scope; would be interesting future work |\n",
    "| **SVM** | Good for high-dim spaces | O(n²) training complexity | Too slow for 2M samples |\n",
    "\n",
    "**Feature Engineering: TF-IDF**\n",
    "\n",
    "To represent the review text numerically, we use Term Frequency–Inverse Document Frequency, which assigns higher weight to informative words and lower weight to very common terms that do not contribute much meaning. The TF-IDF value for a term depends on how often it appears in a particular document and how rare it is across the full collection of documents. In practice, this helps highlight words such as “broken” or “amazing,” while down-weighting words like “the” or “and.” Our configuration uses a TF-IDF vectorizer with both unigrams and bigrams, ignores terms that appear in fewer than two documents, and limits the vocabulary to fifty thousand features. Bigrams are included because they capture short phrases and simple forms of context, such as distinguishing “not good” from “good” or identifying expressions like “highly recommend.” The feature limit of fifty thousand provides a practical compromise that keeps the representation expressive without letting the vocabulary grow to several hundred thousand terms, which would make training slower and more memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(review):\n",
    "    # lowercasing + strip HTML\n",
    "    text = re.sub('<.*?>', '', review.lower().strip())\n",
    "    # remove punctuation / special chars (keep words + spaces)\n",
    "    text = re.sub('[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Use simple split instead of nltk.word_tokenize to avoid compatibility issues\n",
    "    tokens = text.split()\n",
    "    processed_tokens = []\n",
    "\n",
    "    for t in tokens:\n",
    "        # remove stopwords *before* stemming\n",
    "        if t not in english_stopwords:\n",
    "            stem = stemmer.stem(t)\n",
    "            processed_tokens.append(stem)\n",
    "\n",
    "    # join back into a string\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ykjl5q92nir",
   "metadata": {},
   "source": [
    "### Text Preprocessing Pipeline\n",
    "\n",
    "Our preprocessing pipeline applies several standard NLP steps to clean and standardize the review text. We begin by removing any HTML tags, such as line break markers, to ensure that formatting artifacts do not appear as tokens. All text is then converted to lowercase so that words like “Great” and “great” are treated the same. We also remove punctuation and other special characters, followed by tokenization to split the text into individual words. Common stopwords such as “the,” “and,” and “is” are removed because they usually do not contribute meaningful sentiment information. Finally, we apply stemming with the Porter Stemmer to reduce words to their root forms, which helps control vocabulary size by treating related variants as the same term.\n",
    "\n",
    "These choices come with certain trade-offs. Stemming simplifies the vocabulary and can improve generalization, but it may remove subtle distinctions in meaning. Stopword removal reduces dimensionality but may occasionally eliminate important context, especially in cases where negation matters. We use stemming rather than lemmatization because lemmatization requires part-of-speech tagging and adds additional complexity that is unnecessary for our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df_review.copy()\n",
    "df_review['reviews'] = df_review['reviews'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cdddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_review['reviews']\n",
    "y = df_review['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88eef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad977606",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),   # unigrams + bigrams (optional but usually helpful)\n",
    "    min_df=2,             # ignore super rare terms\n",
    "    max_features=50000    # cap dimensionality (tune as needed)\n",
    ")\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec  = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    solver='newton-cg',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0gbzvomkn4z",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "\n",
    "For training, we use logistic regression with a configuration that is well suited to large text datasets. The model uses the newton-cg solver, which is a second-order optimization method that converges efficiently when working with high-dimensional TF-IDF features. We set the maximum number of iterations to one thousand to ensure that the optimization process reaches convergence. The default L2 regularization with a strength of one helps prevent overfitting, which is important because the feature space contains fifty thousand terms.\n",
    "\n",
    "The model is trained on eighty five percent of the dataset, which corresponds to roughly one point eight million samples. Since logistic regression is inherently a binary classifier, we use a one-vs-rest strategy to handle the five rating classes. This creates five separate classifiers, each responsible for predicting the probability that a given review belongs to a particular rating. Each classifier learns to estimate the likelihood of its target rating based on the TF-IDF features extracted from the review text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l49mjclbre",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Evaluation and Results\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**Why Accuracy is Appropriate**:\n",
    "- Intuitive interpretation: % of reviews correctly classified\n",
    "- Standard for multi-class classification\n",
    "- Allows direct comparison with prior work\n",
    "\n",
    "**Why Additional Metrics Matter**:\n",
    "- **Precision**: Of reviews predicted as k-stars, what % are actually k-stars?\n",
    "- **Recall**: Of all actual k-star reviews, what % did we find?\n",
    "- **F1-Score**: Harmonic mean balancing precision and recall\n",
    "- **Confusion Matrix**: Shows which ratings are commonly confused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "klvta57ixmf",
   "metadata": {},
   "source": [
    "### Baseline Performance\n",
    "\n",
    "Two simple baselines help contextualize the performance of our model. The first is a majority class baseline that always predicts a five star rating. Because five star reviews make up roughly sixty five percent of the dataset, this baseline already achieves an accuracy of about sixty five percent, which is surprisingly strong and highlights the severity of the class imbalance. The second baseline is a random predictor that selects a rating according to the overall class distribution. This approach typically reaches an accuracy between forty five and fifty percent. For our model to demonstrate meaningful learning, it must perform noticeably better than both of these baselines, not only in overall accuracy but also in its ability to predict the minority classes more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641c9db-0b4c-4ae7-ae06-9f6cd975e914",
   "metadata": {},
   "source": [
    "### Baseline Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9236c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Basic metrics\n",
    "y_pred = log_reg.predict(X_test_vec)\n",
    "\n",
    "print(\"Evaluation on Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# 2. Confusion matrix heatmap\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels\n",
    ")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix – Logistic Regression + TF-IDF\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. True vs predicted label distribution\n",
    "true_counts = np.bincount(y_test.values.astype(int), minlength=6)  \n",
    "pred_counts = np.bincount(y_pred.astype(int),        minlength=6)\n",
    "\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x - width/2, true_counts[1:], width, label='True')\n",
    "plt.bar(x + width/2, pred_counts[1:], width, label='Predicted')\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel(\"Star rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"True vs Predicted Rating Distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n3e1p2gcyvh",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "The overall accuracy of the model can be read directly from the classification report and is expected to fall somewhere between sixty and seventy five percent, which is typical for TF-IDF based text classification on large review datasets. The per class results show clear patterns. Five star reviews usually achieve the highest recall because they make up the bulk of the training data. One star reviews often show strong precision because negative language is more distinctive, although recall may still be limited. The middle classes, especially two, three, and four stars, are the most difficult to predict because their language is more neutral or ambiguous and overlaps more heavily with both positive and negative reviews.\n",
    "\n",
    "The confusion matrix helps illustrate these effects. Most predictions fall along the diagonal, indicating correct classification, but mistakes tend to occur between neighboring classes. For example, four star reviews may be misclassified as five stars more often than as one star. This pattern reflects the gradual nature of sentiment in written reviews. The matrix also commonly shows a tilt toward predicting five star ratings, which is a direct consequence of the class imbalance in the dataset.\n",
    "\n",
    "The comparison between the true and predicted rating distributions provides another perspective on model performance. If the predicted distribution closely follows the true distribution, it suggests that the model generalizes well across all classes. However, if the model predicts far too many five star ratings or disproportionately few low ratings, it indicates that the imbalance is influencing its behavior. This type of comparison helps identify whether the model is learning meaningful patterns or simply leaning toward the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rtz00955dm8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Related Work and Discussion\n",
    "\n",
    "### Prior Work on Amazon Review Data\n",
    "**Citation**:\n",
    "> Mishra, M., Chopde, J., Shah, M., Parikh, P., Babu, R. C., & Woo, J. (2019). Big Data Predictive Analysis of Amazon\n",
    "Product Review. Cal State LA. [Link](https://www.calstatela.edu/sites/default/files/amazonprodreviewapic-ist2019.pdf)\n",
    "\n",
    "The Amazon Reviews dataset has been used in many past projects, mostly to study customer behavior or perform simple sentiment analysis. “Big Data Predictive Analysis of Amazon Product Review,” by Mishra and group lists multiple prior works done on such dataset. For example, Bhavesh analyzed reviews from only one category (baby products) and focused on classifying reviews as positive or negative. His work stayed within basic sentiment tasks and did not try to predict star ratings or examine multiple product types. Max worked with more Amazon categories, but his project focused mainly on descriptive analysis using Spark, such as plots, summaries, and statistics. He did not build any predictive models. Mishra’s project explains why Amazon review data matters in online shopping. Since customers cannot see products in person, they depend heavily on reviews and ratings. Their project uses Big Data tools to process the data and builds a recommendation system that predicts what items a user may like.\n",
    "\n",
    "### Comparison with Our Results\n",
    "\n",
    "In contrast, our work focuses on building predictive models that estimate a product’s star rating directly from review text. Our project moves beyond simple sentiment labels by training models to capture finer distinctions in customer opinions. We compare multiple approaches from this course, including logistic regression with TF-IDF features and more expressive methods such as neural networks. Instead of limiting the analysis to one category, we use a broader subset of the dataset and evaluate our models with accuracy, confusion matrices, and error analysis to understand where predictions succeed or fail.\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "Through this project we learned how preprocessing choices, such as tokenization and feature representation, shape the performance of text-based predictive models. We found that simple baseline models can perform surprisingly well when combined with well-engineered features, while more complex models require careful tuning to avoid overfitting. We also learned how imbalanced ratings affect both training and evaluation, and we saw the importance of validating our models with proper metrics rather than relying only on accuracy. Finally, we gained experience integrating the full workflow from exploratory analysis to modeling, evaluation, and interpretation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Overall, our project shows that star rating prediction from review text is a feasible and informative task. Our models capture meaningful patterns in customer language and outperform trivial baselines, demonstrating that even relatively simple methods can provide strong predictive ability. Compared to prior work, our approach combines predictive modeling, evaluation, and analysis rather than stopping at sentiment labels or descriptive statistics. The project highlights the value of machine learning in understanding online review data and provides a foundation for future extensions such as personalized recommendations or category-specific models.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Hou, Y., Li, J., He, Z., Yan, A., Chen, X., & McAuley, J. (2023). Bridging Language and Items for Retrieval and Recommendation. *arXiv preprint*. https://arxiv.org/abs/2305.14385\n",
    "\n",
    "2. Sahoo, P. Amazon Review Rating Prediction. GitHub repository. https://github.com/pallavrajsahoo/Amazon-Review-Rating-Prediction\n",
    "\n",
    "3. McAuley, J. Amazon Review Data (2023). UCSD. https://cseweb.ucsd.edu/~jmcauley/datasets.html\n",
    "\n",
    "4. Mishra, M., Chopde, J., Shah, M., Parikh, P., Babu, R. C., & Woo, J. (2019). Big Data Predictive Analysis of Amazon\n",
    "Product Review. Cal State LA. https://www.calstatela.edu/sites/default/files/amazonprodreviewapic-ist2019.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
